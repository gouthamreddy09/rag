ğŸ“„ RAG Pipeline â€“ Retrieval-Augmented Generation for PDFs

ğŸš€ Overview

This project implements a Retrieval-Augmented Generation (RAG) pipeline in Google Colab. It processes PDF documents, extracts relevant information, and provides intelligent responses by combining document chunking, vector embeddings, semantic search, and LLM-based generation.

The pipeline enables document-based Q&A, knowledge retrieval, and intelligent search with traceability back to source documents.

â¸»

ğŸ—ï¸ System Architecture

Pipeline Flow:
	1.	PDF Processing â€“ Extract text & metadata using pdfplumber.
	2.	Text Chunking â€“ Split documents into chunks with LangChainâ€™s RecursiveCharacterTextSplitter.
	3.	Embeddings â€“ Encode chunks with SentenceTransformers (paraphrase-mpnet-base-v2).
	4.	Vector Search â€“ Store & query embeddings using FAISS.
	5.	LLM Response Generation â€“ Use TinyLlama-1.1B-Chat for contextual responses.

â¸»

ğŸ› ï¸ Key Technologies
	â€¢	PDF Processing: pdfplumber
	â€¢	Chunking: LangChain
	â€¢	Embeddings: SentenceTransformers
	â€¢	Vector DB: FAISS
	â€¢	LLM: TinyLlama-1.1B-Chat
	â€¢	Other: scikit-learn, numpy, Pillow

â¸»

âš™ï¸ Installation & Setup

# Install dependencies
pip install pdfplumber langchain sentence-transformers faiss-cpu scikit-learn numpy transformers accelerate pillow

Authentication

To use Hugging Face models:

from huggingface_hub import login
login()  # Requires HuggingFace token


â¸»

ğŸ”‘ Core Functions

1. PDF Text Extraction

page_data = load_pdf_text_with_metadata("sample.pdf")

	â€¢	Extracts text with bounding boxes & page metadata.
	â€¢	Supports multi-page documents.

2. Text Chunking

chunks = chunk_text_with_metadata(page_data, chunk_size=800, chunk_overlap=100)

	â€¢	Preserves structure & position info.

3. Embeddings & Indexing

embedding_model, faiss_index, chunks = embed_and_index_chunks(chunks)

	â€¢	Embeds text using SentenceTransformers.
	â€¢	Stores vectors in FAISS index.

4. Semantic Search + LLM

search("What is the pipeline architecture?", embedding_model, faiss_index, chunks)

	â€¢	Retrieves top-k relevant chunks.
	â€¢	Generates contextual response with TinyLlama.

â¸»

ğŸ§‘â€ğŸ’» Usage

Basic Flow

# 1. Upload PDFs
page_data = load_pdf_text_with_metadata("document.pdf")

# 2. Chunking
chunks = chunk_text_with_metadata(page_data)

# 3. Embeddings & Index
embedding_model, faiss_index, chunks = embed_and_index_chunks(chunks)

# 4. Query
search("Explain the core components", embedding_model, faiss_index, chunks)

Advanced Options
	â€¢	Custom chunking: Adjust chunk_size and chunk_overlap.
	â€¢	Search parameters: Tune top_k and min_score.
	â€¢	Visual feedback: Highlight retrieved chunks on PDF images.

â¸»

ğŸŒŸ Features
	â€¢	âœ… Semantic PDF search with context-aware responses
	â€¢	âœ… Visual highlighting of relevant PDF sections
	â€¢	âœ… Multi-document processing
	â€¢	âœ… Persistent storage with pickle
	â€¢	âœ… Conversation analysis for transcripts

â¸»

ğŸ Troubleshooting
	â€¢	Font warnings â†’ Cosmetic, can be ignored.
	â€¢	Memory issues â†’ Reduce chunk_size or process in batches.
	â€¢	Low relevance scores â†’ Lower min_score or refine query.
	â€¢	Model errors â†’ Ensure HuggingFace authentication & internet access.

â¸»

ğŸ“Š Performance
	â€¢	Document processing: ~2â€“5 sec/page
	â€¢	Embedding generation: ~1â€“2 sec/chunk
	â€¢	Search latency: <100ms (typical queries)
	â€¢	Memory usage: ~2â€“4GB (moderate docs)

â¸»

ğŸ“Œ Best Practices
	â€¢	Use PDFs with extractable text (not scanned images).
	â€¢	Write specific queries for better retrieval.
	â€¢	Tune chunk size & overlap based on document type.
	â€¢	Validate search results before production use.

â¸»

ğŸ”® Future Enhancements
	â€¢	Larger document scaling
	â€¢	Domain-specific fine-tuning
	â€¢	Advanced retrieval + re-ranking
	â€¢	Production system integration

â¸»

ğŸ“– License

This project is released under the MIT License.


